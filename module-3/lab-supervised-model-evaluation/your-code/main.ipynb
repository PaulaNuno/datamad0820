{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning Model Evaluation Lab\n",
    "\n",
    "Complete the exercises below to solidify your knowledge and understanding of supervised learning model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "data = load_boston()\n",
    "\n",
    "X = pd.DataFrame(data[\"data\"], columns=data[\"feature_names\"])\n",
    "y = pd.DataFrame(data[\"target\"], columns=['MEDV'])\n",
    "\n",
    "data = pd.concat([X, y], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split this data set into training (80%) and testing (20%) sets.\n",
    "\n",
    "The `MEDV` field represents the median value of owner-occupied homes (in $1000's) and is the target variable that we will want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13) (102, 13) (404, 1) (102, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a `LinearRegression` model on this data set and generate predictions on both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_reg_model = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "pred_train = linear_reg_model.predict(X_train)\n",
    "pred_test = linear_reg_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate and print R-squared for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared for training: 0.7262754203742635\n",
      "R-squared for testing:0.7897577126614312\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score  \n",
    "\n",
    "r_squ_train = r2_score(y_train, pred_train)\n",
    "r_squ_test = r2_score(y_test, pred_test)\n",
    "\n",
    "print(f\"R-squared for training: {r_squ_train}\\nR-squared for testing:{r_squ_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate and print mean squared error for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for training: 23.420384258168646\n",
      "Mean squared error for testing: 16.791682090556886\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squ_error_train = mean_squared_error(y_train, pred_train)\n",
    "mean_squ_error_test = mean_squared_error(y_test, pred_test)\n",
    "\n",
    "print(f\"Mean squared error for training: {mean_squ_error_train}\\nMean squared error for testing: {mean_squ_error_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate and print mean absolute error for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error for training: 3.3961794769478986\n",
      "Mean absolute error for testing: 3.19358409655647\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_abs_error_train = mean_absolute_error(y_train, pred_train)\n",
    "mean_abs_error_test = mean_absolute_error(y_test, pred_test)\n",
    "\n",
    "print(f\"Mean absolute error for training: {mean_abs_error_train}\\nMean absolute error for testing: {mean_abs_error_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "\n",
    "X = pd.DataFrame(data[\"data\"], columns=data[\"feature_names\"])\n",
    "y = pd.DataFrame(data[\"target\"], columns=[\"class\"])\n",
    "\n",
    "data = pd.concat([X, y], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Split this data set into training (80%) and testing (20%) sets.\n",
    "\n",
    "The `class` field represents the type of flower and is the target variable that we will want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (30, 4) (120, 1) (30, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train a `LogisticRegression` model on this data set and generate predictions on both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paula/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/paula/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_regr_model = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "pred_train =log_regr_model.predict(X_train)\n",
    "pred_test =log_regr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Calculate and print the accuracy score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for training: 0.9666666666666667\n",
      "Accuracy score for testing: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_score_train = accuracy_score(y_train, pred_train)\n",
    "acc_score_test = accuracy_score(y_test, pred_test)\n",
    "\n",
    "print (f'Accuracy score for training: {acc_score_train}\\nAccuracy score for testing: {acc_score_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Calculate and print the balanced accuracy score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The balanced accuracy score for training is 0.9666666666666667 and for testing 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "balan_acc_score_train = balanced_accuracy_score(y_train, pred_train)\n",
    "balan_acc_score_test = balanced_accuracy_score(y_test, pred_test)\n",
    "\n",
    "print (f'The balanced accuracy score for training is {balan_acc_score_train} and for testing {balan_acc_score_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Calculate and print the precision score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision score for training is 0.967418546365915 and for testing 0.9696969696969697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paula/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1267: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "pre_score_train = precision_score(y_train, pred_train, pos_label='positive', average='macro')\n",
    "pre_score_test = precision_score(y_test, pred_test, pos_label='positive', average='macro')\n",
    "\n",
    "print (f'The precision score for training is {pre_score_train} and for testing {pre_score_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Calculate and print the recall score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recall score for training is 0.9666666666666667 and for testing 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "recall_score_train = recall_score(y_train, pred_train, average='macro')\n",
    "recall_score_test = recall_score(y_test, pred_test, average='macro')\n",
    "\n",
    "print (f'The recall score for training is {recall_score_train} and for testing {recall_score_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Calculate and print the F1 score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score for training is 0.966645820304357 and for testing 0.9665831244778613\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score_train = f1_score(y_train, pred_train, average='macro')\n",
    "f1_score_test = f1_score(y_test, pred_test, average='macro')\n",
    "\n",
    "print (f'The F1 score for training is {f1_score_train} and for testing {f1_score_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Generate confusion matrices for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40,  0,  0],\n",
       "       [ 0, 37,  3],\n",
       "       [ 0,  1, 39]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "train_conf_matrix =confusion_matrix(y_train, pred_train)\n",
    "train_conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  0,  0],\n",
       "       [ 0, 10,  0],\n",
       "       [ 0,  1,  9]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conf_matrix = confusion_matrix(y_test, pred_test)\n",
    "test_conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: For each of the data sets in this lab, try training with some of the other models you have learned about, recalculate the evaluation metrics, and compare to determine which models perform best on each data set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
